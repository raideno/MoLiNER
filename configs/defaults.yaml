# @package __global__

run_dir: logs

hydra:
  run:
    dir: ${run_dir}
  # TODO: added to support multirun
  sweep:
    dir: ${run_dir}

seed: 1234
logger_level: INFO

loggers:
  csv:
    _target_: src.logger.csv.CSVLogger
    save_dir: ${run_dir}
    name: logs
  tensorboard:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: ${run_dir}
    name: logs
  wandb:
    _target_: src.logger.wandb.WandBLogger
    project: moliner
    name: ${run_dir}
    save_dir: ${run_dir}
    offline: false
    tags: []
    notes: "MoLiNER training run"
    # NOTE: set to your WandB username or team
    entity: nadirkichou-university-of-lille

callbacks:
  early_stopping:
    _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
    monitor: val/loss
    patience: 32
    mode: min
    min_delta: 0.001
    verbose: true
  model_checkpoint_best:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    filename: best-{epoch}
    monitor: val/loss
    mode: min
    save_top_k: 4
    save_last: true
  model_checkpoint_latest:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    filename: latest-{epoch}
    every_n_epochs: 1
    save_top_k: 1
  visualization:
    _target_: src.callbacks.visualization.VisualizationCallback
    batch_index: 0
    debug: false
    # NOTE: run every 5 epochs instead of every epoch
    dirpath: ${run_dir}/visualizations

defaults:
  - _self_
  - override hydra/job_logging: tqdm
  - override hydra/hydra_logging: tqdm
