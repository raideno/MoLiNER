trainer:
  _target_: pytorch_lightning.Trainer

  max_epochs: 256
  log_every_n_steps: 50
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1
  # NOTE: options: cuda, cpu, mps (for Apple Silicon), auto
  accelerator: auto
  # NOTE: auto, [0], [1], [0,1], etc. or specific device IDs
  # devices: [1]

  profiler:
    _target_: pytorch_lightning.profilers.SimpleProfiler
    dirpath: ${run_dir}/logs
    filename: profiling.txt

  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      filename: best-{epoch}
      monitor: val/loss
      mode: min
      save_top_k: 4
      save_last: true
    # NOTE: save every one epoch
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      filename: latest-{epoch}
      every_n_epochs: 1
      # NOTE: keep only the best checkpoint based on the monitored metric (val_los)
      save_top_k: 1
      save_last: true
    - _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
      monitor: val/loss
      patience: 32
      mode: min
      min_delta: 0.001
      verbose: true
    - _target_: src.callbacks.progress.ProgressLogger
      precision: 3
    - _target_: src.callbacks.tqdmbar.TQDMProgressBar
    # NOTE: custom callback for visualization every epoch
    - _target_: src.callbacks.visualization.VisualizationCallback
      batch_index: 0
      num_samples: 2
      # NOTE: can be a single value or list of thresholds
      score_threshold: [0.3, 0.5, 0.7]
      debug: false
      visualize_on_start: true
      visualize_on_end: true
      dirpath: ${run_dir}/visualizations

  logger:
    _target_: src.logger.csv.CSVLogger
    save_dir: ${run_dir}
    name: logs
