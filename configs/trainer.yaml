trainer:
  _target_: pytorch_lightning.Trainer

  max_epochs: 256
  # NOTE: log every training step
  log_every_n_steps: 1
  num_sanity_val_steps: 2
  check_val_every_n_epoch: 1
  # NOTE: options: cuda, cpu, mps (for Apple Silicon), auto
  accelerator: ???
  # NOTE: auto, [0], [1], [0,1], etc. or specific device IDs
  # devices: [1]

  # NOTE: will run on only 2 batches and the same batches will be used for validation, training and testing. Useful for debugging
  # https://pytorch-lightning.readthedocs.io/en/1.0.8/debugging.html
  # overfit_batches: 2

  profiler:
    _target_: pytorch_lightning.profilers.SimpleProfiler
    dirpath: ${run_dir}/logs
    filename: profiling.txt

  callbacks:
    - ${callbacks.model_checkpoint_best}
    - ${callbacks.model_checkpoint_latest}
    # - ${callbacks.early_stopping}
    # - ${callbacks.visualization}

  logger:
    - ${loggers.csv}
    # - ${loggers.tensorboard}
    - ${loggers.wandb}
