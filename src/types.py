import enum
import torch
import typing
import random
import string
import dataclasses
import transformers

# Use TYPE_CHECKING to avoid circular imports
if typing.TYPE_CHECKING:
    from src.model.modules import BasePromptsTokensEncoder
    
class MotionDict(typing.TypedDict):
    new_joints: typing.List[typing.List[float]]
    new_joint_vecs: typing.List[typing.List[float]]

class PromptsDict(typing.TypedDict):
    text: str
    span: typing.Tuple[int, int]
    source: typing.List[str]
    is_sequence: bool
    
class DatasetSample(typing.TypedDict):
    sid: str | int
    motion: MotionDict
    prompts: typing.List[PromptsDict]
    amass_file_relative_path: str

@dataclasses.dataclass
class RawBatch:
    sid: typing.List[int] | typing.List[str] | typing.List[str | int]
    dataset_name: typing.List[str]
    amass_relative_path: typing.List[str]
    
    # NOTE: (batch_size, batch_max_frames_per_motion, 22, 3)
    raw_motion: torch.Tensor
    # NOTE: (batch_size, batch_max_frames_per_motion, 263)
    transformed_motion: torch.Tensor
    # NOTE: (batch_size, batch_max_frames_per_motion)
    # For each motion, set to 1 if the motion is valid, 0 otherwise (padding frame).
    motion_mask: torch.Tensor
    
    # NOTE: a list of lists, each inner list contains the prompts for the corresponding motion
    # A prompt is now a tuple of (text, list_of_spans, is_sequence_prompt)
    # where list_of_spans is a list of (start_frame, end_frame) tuples for this prompt
    # The is_sequence_prompt flag indicates if the prompt is a sequence-level prompt (True) or a frame-level prompt (False).
    prompts: typing.List[
        typing.List[
            typing.Tuple[
                str, typing.List[typing.Tuple[int, int]], bool
            ]
        ]
    ]
    
    # --- Processed Prompt Data (The "Labels" to be localized) ---
    
    # NOTE: (batch_size, batch_max_prompts_per_motion, batch_max_prompt_length_in_tokens)
    # The tokenized and padded text prompts.
    prompt_input_ids: typing.Optional[torch.Tensor] = None

    # NOTE: (batch_size, batch_max_prompts_per_motion, batch_max_prompt_length_in_tokens)
    # The attention mask for the tokenized prompts. A 1 indicates a real token, 0 indicates padding.
    prompt_attention_mask: typing.Optional[torch.Tensor] = None
    
    # --- Ground Truth Data (The Localizations of each prompt) ---
    
    # NOTE:: (batch_size, batch_max_prompts_per_motion, batch_max_spans_per_prompt, 2)
    # The last dim is [start_frame, end_frame]
    # Only required for training
    target_spans: typing.Optional[torch.Tensor] = None

    # NOTE: (batch_size, batch_max_prompts_per_motion)
    # A mask to indicate which prompts are real vs. padding, as each motion in the batch can have a different number of prompts.
    # Only required for training
    target_spans_mask: typing.Optional[torch.Tensor] = None
    
    # NOTE: (batch_size, batch_max_prompts_per_motion, batch_max_spans_per_prompt)
    # A mask to indicate which spans within each prompt are real vs. padding
    # Only required for training
    target_spans_per_prompt_mask: typing.Optional[torch.Tensor] = None
    
    # NOTE: (batch_size, batch_max_prompts_per_motion)
    # A boolean tensor indicating if the prompt is a sequence-level prompt (True) or a frame-level prompt (False).
    # Only required for training
    is_sequence_prompt: typing.Optional[torch.Tensor] = None
    
    @property
    def motion_features(self) -> torch.Tensor:
        return self.transformed_motion

    def to(self, device: torch.device) -> "RawBatch":
        return RawBatch(
            sid=self.sid,
            dataset_name=self.dataset_name,
            amass_relative_path=self.amass_relative_path,
            raw_motion=self.raw_motion.to(device),
            transformed_motion=self.transformed_motion.to(device),
            motion_mask=self.motion_mask.to(device),
            prompts=self.prompts,
            prompt_input_ids=self.prompt_input_ids.to(device) if self.prompt_input_ids is not None else None,
            prompt_attention_mask=self.prompt_attention_mask.to(device) if self.prompt_attention_mask is not None else None,
            target_spans=self.target_spans.to(device) if self.target_spans is not None else None,
            target_spans_mask=self.target_spans_mask.to(device) if self.target_spans_mask is not None else None,
            target_spans_per_prompt_mask=self.target_spans_per_prompt_mask.to(device) if self.target_spans_per_prompt_mask is not None else None,
            is_sequence_prompt=self.is_sequence_prompt.to(device) if self.is_sequence_prompt is not None else None
        )

@dataclasses.dataclass
class MolinerForwardOutput:
    # The core output matrix from the pair scorer.
    # NOTE: (batch_size, batch_max_prompts_per_motion, batch_max_spans_per_motion_in_batch)
    similarity_matrix: torch.Tensor

    # The start and end frames for each candidate span generated by the model.
    # NOTE: (batch_size, batch_max_spans_per_motion_in_batch, 2) -> [start_frame, end_frame]
    candidate_spans_indices: torch.Tensor
    
    # A mask to identify which candidate spans are valid versus padding.
    # NOTE: (batch_size, batch_max_spans_per_motion_in_batch)
    candidate_spans_mask: torch.Tensor
    
    # --- --- --- xxx --- --- ---
    
    prompts_mask: torch.Tensor
            
    # --- --- --- xxx --- --- ---
    
@dataclasses.dataclass
class SegmenterForwardOutput:
    # [windows_in_batch, num_classes]
    class_logits: torch.Tensor
    
    # [windows_in_batch, 1]
    start_logits: torch.Tensor
    
    # [windows_in_batch, 1]
    end_logits: torch.Tensor
    
    # [windows_in_batch, 3]; 3 for batch_index, start_frame, end_frame
    windows_positions: torch.Tensor
    
    # [batch_size, ]
    windows_per_sample: torch.Tensor
    
    batch_size: int
    
@dataclasses.dataclass
class EvaluationResult:
    """
    Holds the results of a single motion evaluation.
    """
    motion_length: typing.List[int]
    predictions: typing.List[
        typing.List[ 
            typing.Tuple[
                # NOTE: prompt text and a list of associated spans with their scores
                str, typing.List[typing.Tuple[int, int, float]]
            ]
        ]
    ]
