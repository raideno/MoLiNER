import torch
import typing

from ._base import BaseSpanRepresentationLayer

class SpanQueryRepresentationLayer(BaseSpanRepresentationLayer):
    """
    Based on the SpanQuery implementation from GLiNER.
    Learns a unique "query vector" for each possible span width.
    The representation for a span is generated by a combination (matrix multiplication) of its start token's embedding and the query vector corresponding to its width.
    """
    
    def __init__(self, motion_embed_dim: int, representation_dim: int, max_span_width: int, min_span_width: int = 1):
        """
        Initializes the SpanQueryRepresentationLayer.
        
        Args:
            motion_embed_dim (int): The dimension of the motion frame embeddings.
            representation_dim (int): The dimension of the final output representation.
            max_span_width (int): Maximum span width to support.
            min_span_width (int): Minimum span width to support.
        """
        super().__init__()
        
        if not isinstance(max_span_width, int) or max_span_width < 1:
            raise ValueError("max_span_width must be a positive integer.")
        if not isinstance(min_span_width, int) or min_span_width < 1:
            raise ValueError("min_span_width must be a positive integer.")
        if min_span_width > max_span_width:
            raise ValueError("min_span_width must be less than or equal to max_span_width.")
        
        self.max_span_width = max_span_width
        self.min_span_width = min_span_width
        
        # NOTE: learnable query vectors for each span width from min_span_width to max_span_width
        span_range = max_span_width - min_span_width + 1
        self.query_seg = torch.nn.Parameter(torch.randn(motion_embed_dim, span_range))
        
        torch.nn.init.uniform_(self.query_seg, a=-1, b=1)
        
        self.project = torch.nn.Sequential(
            torch.nn.Linear(motion_embed_dim, representation_dim),
            torch.nn.ReLU()
        )
    
    def forward(
        self,
        motion_features: torch.Tensor,
        span_indices: torch.Tensor,
        spans_masks: torch.Tensor,
        batch_index: typing.Optional[int] = None,
    ) -> torch.Tensor:
        """
        Forward pass using query-based span representation.
        
        Args:
            motion_features: (batch_size, seq_len, embed_dim)
            span_indices: (batch_size, max_spans, 2) 
            spans_masks: (batch_size, max_spans)
            
        Returns:
            torch.Tensor: (batch_size, max_spans, representation_dim)
        """
        batch_size, max_spans, _ = span_indices.shape
        seq_len, embed_dim = motion_features.shape[1], motion_features.shape[2]
        
        # NOTE: motion_features: [B, L, D], query_seg: [D, span_range]
        all_span_reps = torch.einsum('bld,ds->blsd', motion_features, self.query_seg)
        # NOTE: [B, L, span_range, D]
        all_span_reps = self.project(all_span_reps)
        
        span_representations = torch.zeros(
            batch_size, max_spans, all_span_reps.shape[-1], 
            device=motion_features.device
        )
        
        # NOTE: in order to handle padding, we only fill span representations for valid spans
        # As previously we have computed all possible span representations, not only the ones given by the span indices.
        for i in range(batch_size):
            for j in range(max_spans):
                if spans_masks[i, j]:
                    start, end = span_indices[i, j]
                    span_width = end - start + 1
                    
                    if self.min_span_width <= span_width <= self.max_span_width and start >= 0:
                        query_index = span_width - self.min_span_width
                        span_representations[i, j] = all_span_reps[i, start, query_index]
        
        span_representations = span_representations * spans_masks.unsqueeze(-1)
        
        return span_representations
