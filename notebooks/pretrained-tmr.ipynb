{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffad775",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c8a9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "    print(\"[sys.path]:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25f7a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.model.actor import ACTORStyleEncoder, ACTORStyleDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92dc51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOTION_DECODER_WEIGHTS_PATH = \"/home/nadir/motion-linner/models/tmr_humanml3d_guoh3dfeats/last_weights/motion_decoder.pt\"\n",
    "MOTION_ENCODER_WEIGHTS_PATH = \"/home/nadir/motion-linner/models/tmr_humanml3d_guoh3dfeats/last_weights/motion_encoder.pt\"\n",
    "TEXT_ENCODER_WEIGHTS_PATH = \"/home/nadir/motion-linner/models/tmr_humanml3d_guoh3dfeats/last_weights/text_encoder.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77e8c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_token_emb:\n",
    "#   _target_: src.data.text.TokenEmbeddings\n",
    "#   path: datasets/annotations/${hydra:runtime.choices.data}\n",
    "#   modelname: distilbert-base-uncased\n",
    "#   preload: true\n",
    "\n",
    "# text_to_sent_emb:\n",
    "#   _target_: src.data.text.SentenceEmbeddings\n",
    "#   path: datasets/annotations/${hydra:runtime.choices.data}\n",
    "#   modelname: sentence-transformers/all-mpnet-base-v2\n",
    "#   preload: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ca7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text processing setup complete - ready to use with TMR text encoder\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(\"Text processing setup complete - ready to use with TMR text encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e211b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_encoder = ACTORStyleEncoder(\n",
    "    nfeats=263,\n",
    "    vae=True,\n",
    "    latent_dim=256,\n",
    "    ff_size=1024,\n",
    "    num_layers=6,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    activation='gelu'\n",
    ")\n",
    "motion_encoder.load_state_dict(\n",
    "    torch.load(MOTION_ENCODER_WEIGHTS_PATH, map_location='cpu')\n",
    ")\n",
    "\n",
    "text_encoder = ACTORStyleEncoder(\n",
    "    nfeats=768,\n",
    "    vae=True,\n",
    "    latent_dim=256,\n",
    "    ff_size=1024,\n",
    "    num_layers=6,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    activation='gelu'\n",
    ")\n",
    "text_encoder.load_state_dict(\n",
    "    torch.load(TEXT_ENCODER_WEIGHTS_PATH, map_location='cpu')\n",
    ")\n",
    "\n",
    "motion_decoder = ACTORStyleDecoder(\n",
    "    nfeats=263,\n",
    "    latent_dim=256,\n",
    "    ff_size=1024,\n",
    "    num_layers=6,\n",
    "    num_heads=4,\n",
    "    dropout=0.1,\n",
    "    activation='gelu'\n",
    ")\n",
    "motion_decoder.load_state_dict(\n",
    "    torch.load(MOTION_DECODER_WEIGHTS_PATH, map_location='cpu'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e50a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: torch.Size([1, 77, 768])\n",
      "Text latent shape: torch.Size([1, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "def process_text_for_tmr(text, max_length=77):\n",
    "    \"\"\"Process text input for the TMR model\"\"\"\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = text_model(**tokens).last_hidden_state\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "sample_text = \"a person walks forward and then turns around\"\n",
    "text_features = process_text_for_tmr(sample_text)\n",
    "print(f\"Text features shape: {text_features.shape}\")\n",
    "\n",
    "mask = torch.ones(text_features.shape[:-1], dtype=torch.bool)\n",
    "\n",
    "text_encoder.eval()\n",
    "with torch.no_grad():\n",
    "    text_latent = text_encoder({\"x\": text_features, \"mask\": mask})\n",
    "    print(f\"Text latent shape: {text_latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd953c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.tmr import TMR\n",
    "\n",
    "tmr_model = TMR(\n",
    "    motion_encoder=motion_encoder,\n",
    "    text_encoder=text_encoder,\n",
    "    motion_decoder=motion_decoder,\n",
    "    vae=True,\n",
    "    lmd={\"recons\": 1.0, \"latent\": 1.0e-5, \"kl\": 1.0e-5, \"contrastive\": 0.1},\n",
    "    lr=1e-4,\n",
    "    temperature=0.7,\n",
    "    threshold_selfsim=0.80,\n",
    "    threshold_selfsim_metrics=0.95\n",
    ")\n",
    "\n",
    "print(\"TMR model created successfully!\")\n",
    "print(f\"Motion encoder parameters: {sum(p.numel() for p in motion_encoder.parameters())}\")\n",
    "print(f\"Text encoder parameters: {sum(p.numel() for p in text_encoder.parameters())}\")\n",
    "print(f\"Motion decoder parameters: {sum(p.numel() for p in motion_decoder.parameters())}\")\n",
    "\n",
    "tmr_model.eval()\n",
    "motion_encoder.eval()\n",
    "text_encoder.eval()\n",
    "motion_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ce6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_motion_from_text(text, motion_length=120):\n",
    "    \"\"\"Generate motion from text using the pretrained TMR model\"\"\"\n",
    "    \n",
    "    text_features = process_text_for_tmr(text)\n",
    "    \n",
    "    text_x_dict = {\n",
    "        \"x\": text_features,\n",
    "        \"length\": [text_features.shape[1]]\n",
    "    }\n",
    "    \n",
    "    mask = torch.ones(1, motion_length, dtype=torch.bool)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_motion = tmr_model(text_x_dict, mask=mask)\n",
    "    \n",
    "    return generated_motion\n",
    "\n",
    "sample_texts = [\n",
    "    \"a person walks forward\",\n",
    "    \"someone is dancing\",\n",
    "    \"a person sits down and then stands up\",\n",
    "    \"running in a circle\"\n",
    "]\n",
    "\n",
    "print(\"Generating motions from text descriptions...\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    try:\n",
    "        motion = generate_motion_from_text(text)\n",
    "        print(f\"✓ Generated motion {i+1}: '{text}' -> shape: {motion.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error with text {i+1}: '{text}' -> {e}\")\n",
    "\n",
    "print(\"\\nYou can now use the pretrained TMR model for text-to-motion generation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb38cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58017505",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseqTransEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/motion-linner/.motion-linner.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/motion-linner/.motion-linner.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/motion-linner/.motion-linner.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:416\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    389\u001b[0m     src: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     is_causal: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    393\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    394\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Pass the input through the encoder layers in turn.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m        see the docs in :class:`~torch.nn.Transformer`.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     src_key_padding_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m    412\u001b[0m         mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[1;32m    413\u001b[0m         mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc_key_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    414\u001b[0m         other_type\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39m_none_or_dtype(mask),\n\u001b[1;32m    415\u001b[0m         other_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 416\u001b[0m         target_type\u001b[38;5;241m=\u001b[39m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m,\n\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    419\u001b[0m     mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m    420\u001b[0m         mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m    421\u001b[0m         mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m         check_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m     )\n\u001b[1;32m    428\u001b[0m     output \u001b[38;5;241m=\u001b[39m src\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "text_encoder.seqTransEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a6f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _target_: src.model.TEMOS\n",
    "\n",
    "# motion_encoder:\n",
    "#   _target_: src.model.ACTORStyleEncoder\n",
    "#   nfeats: ${data.motion_loader.nfeats}\n",
    "#   vae: true\n",
    "#   latent_dim: 256\n",
    "#   ff_size: 1024\n",
    "#   num_layers: 6\n",
    "#   num_heads: 4\n",
    "#   dropout: 0.1\n",
    "#   activation: gelu\n",
    "\n",
    "# text_encoder:\n",
    "#   _target_: src.model.ACTORStyleEncoder\n",
    "#   nfeats: 768\n",
    "#   vae: true\n",
    "#   latent_dim: 256\n",
    "#   ff_size: 1024\n",
    "#   num_layers: 6\n",
    "#   num_heads: 4\n",
    "#   dropout: 0.1\n",
    "#   activation: gelu\n",
    "\n",
    "# motion_decoder:\n",
    "#   _target_: src.model.ACTORStyleDecoder\n",
    "#   nfeats: ${data.motion_loader.nfeats}\n",
    "#   latent_dim: 256\n",
    "#   ff_size: 1024\n",
    "#   num_layers: 6\n",
    "#   num_heads: 4\n",
    "#   dropout: 0.1\n",
    "#   activation: gelu\n",
    "\n",
    "# vae: true\n",
    "\n",
    "# lmd:\n",
    "#   recons: 1.0\n",
    "#   latent: 1.0e-5\n",
    "#   kl: 1.0e-5\n",
    "\n",
    "# lr: 1e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".motion-linner.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
